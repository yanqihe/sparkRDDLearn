## 作业一使用RDD API实现带词频的倒排索引



### 代码

``` scala
// 获取这个目录下的0.txt, 1.txt, 2.txt
val origFiles = sc.wholeTextFiles("/hsjt/tools/spark-3.1.2-bin-hadoop3.2/workfiles/")

// 只保留文件名和文件内容的映射
val files = origFiles.map(item => {
	val arr = item._1.split("/")
	val key = arr(arr.length - 1).split("\\.")(0)
	(key, item._2.replace("\n", ""))
})
files.sortByKey().collect.foreach(item => println(item._1 + ", " + item._2))
println()

// 拆分每个文件的行
// fileToWords的数据结构为：Array(Map(it -> 0,2, is -> 0,2, what -> 0,1), Map(it -> 2,1, is -> 2,1, a -> 2,1, banana -> 2,1), Map(what -> 1,1, is -> 1,1, it -> 1,1))
val fileToWords = files.map(item => {
	val words = item._2.split(" ")
	
	var wordFile:Map[String, String] = Map()
	for (i <- 0 to words.length - 1) wordFile += (words(i) -> item._1)

	var wordCounts:Map[String, Int] = Map()
	for (i <- 0 to words.length - 1) {
		var isContains = wordCounts.contains( words(i) )
		if (isContains) {
			val count = wordCounts( words(i)) + 1
			wordCounts += (words(i) -> count)
		} else {
			wordCounts += (words(i) -> 1)
		}
	}

	for ( (k, v) <- wordCounts) {
		val newValue =  wordFile(k) + "," + wordCounts(k)
		wordFile += (k -> newValue)
	}
	
	wordFile
})

// 汇总单词与文件名的关联
val keyIndex = fileToWords
	.flatMap(t => t)
	.reduceByKey(_  + " " +  _)

// Key排序
val keySortIndex = keyIndex.sortByKey()

// 带词频反向文件索引
keySortIndex.collect.foreach(item => println(item._1 + ": {" + item._2 + "}"))
```

### 输出的文本日志

 ```tiki wiki
 a: {2,1}
 banana: {2,1}
 is: {0,2 2,1 1,1}
 it: {0,2 2,1 1,1}
 what: {0,1 1,1}
 ```



### 输出的截图

![work_1](image\work_1.png)



### 补充说明

没有对单词出现的文件列表排序